{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#import libraries and secrets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml.html as lh\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow.parquet as pq\n",
    "import boto3\n",
    "import tweepy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = './.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "postgres_user = os.environ['PSQL_USERNAME']\n",
    "postgres_pw = os.environ['PSQL_PASSWORD']\n",
    "postgres_url = 'reporting-db.expapp.com'\n",
    "\n",
    "redshift_user = os.environ['REDSHIFT_USERNAME']\n",
    "redshift_pw = os.environ['REDSHIFT_PASSWORD']\n",
    "redshift_url = os.environ['REDSHIFT_URL']\n",
    "\n",
    "t_consumer_key = os.environ['TWT_CONSUMER_KEY']\n",
    "t_consumer_secret = os.environ['TWT_CONSUMER_SECRET']\n",
    "t_access_token = os.environ['TWT_ACCESS_TOKEN_KEY']\n",
    "t_access_token_secret = os.environ['TWT_ACCESS_TOKEN_SECRET']\n",
    "\n",
    "def load_to_redshift(df_name,db_table_name,pq_str=\"\"):\n",
    "    \n",
    "    parquet_nm='moveit_'+pq_str+'.parquet'\n",
    "    df_name.to_parquet(parquet_nm, index=False)\n",
    "    bashCommand = \"aws s3 cp \"+parquet_nm+\" s3://exp-data-science/\"+parquet_nm\n",
    "\n",
    "    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    \n",
    "    cur.execute(\"select count(*) from \"+db_table_name)\n",
    "    result = cur.fetchone()  \n",
    "    print(\"Current \"+db_table_name+\" row count: \"+str(result[0]))\n",
    "    \n",
    "    statement=\"\"\"\n",
    "    copy {} from 's3://exp-data-science/{}'\n",
    "    iam_role 'arn:aws:iam::xxx:role/Redshift_Copy_Unload'\n",
    "    FORMAT AS PARQUET;\n",
    "    \"\"\".format(db_table_name,parquet_nm)\n",
    "    cur.execute(statement)\n",
    "    \n",
    "    cur.execute(\"select count(*) from \"+db_table_name)\n",
    "    result = cur.fetchone()  \n",
    "    print(\"New \"+db_table_name+\" row count: \"+str(result[0]))\n",
    "    \n",
    "    # authentication of consumer key and secret \n",
    "auth = tweepy.OAuthHandler(t_consumer_key, t_consumer_secret) \n",
    "    \n",
    "# authentication of access token and secret \n",
    "auth.set_access_token(t_access_token, t_access_token_secret) \n",
    "api = tweepy.API(auth) \n",
    "\n",
    "def tweet(txtout):\n",
    "    TARGET_DAY_FORMAT=datetime.today().strftime(\"%c\")\n",
    "    # update the status \n",
    "    api.update_status(\"status | \"+TARGET_DAY_FORMAT+\" \"+txtout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block makes the connection to postgres db (tableau/follower)\n",
    "conn_string = \"host='\"+postgres_url+\"' dbname='db' user='\"+postgres_user+\"' password='\"+postgres_pw+\"'\"\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to make web requests\n",
    "import requests\n",
    "\n",
    "#store the data we get as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#convert the response as a strcuctured json\n",
    "import json\n",
    "\n",
    "#mathematical operations on lists\n",
    "import numpy as np\n",
    "\n",
    "#parse the datetimes we get from NOAA\n",
    "from datetime import datetime\n",
    "\n",
    "from statistics import median \n",
    "\n",
    "#add the access token you got from NOAA\n",
    "Token = 'xxx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bring in data file of zips in every county\n",
    "zips = pd.io.parsers.read_csv('assets/ZIP_COUNTY_032020.csv', dtype={'ZIP': 'str'})\n",
    "#zips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define zipcode foe the venue you need weather for\n",
    "zip_in='54304'\n",
    "\n",
    "#selects county(ies) from zip county file from zip\n",
    "county=zips.loc[zips['ZIP']==zip_in]\n",
    "select_county=county.iloc[0]['COUNTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selects all zips in counties you selected above\n",
    "select_zips=zips.loc[zips['COUNTY']==select_county]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create api call pair for each zip\n",
    "zip_list=select_zips.ZIP.unique()\n",
    "zips_str='locationid=ZIP:'+'&locationid=ZIP:'.join(zip_list)\n",
    "#print(zips_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble api call\n",
    "base='https://www.ncdc.noaa.gov/cdo-web/api/v2/data?'\n",
    "#daily average data set\n",
    "datasetid='datasetid=GHCND'\n",
    "#datatypeid='datatypeid=TAVG'\n",
    "#limit='limit=1000'\n",
    "#stationid=zips_str\n",
    "locationid=zips_str\n",
    "#start date and end date of data pull\n",
    "startdate='startdate=2020-05-29'\n",
    "enddate='enddate=2020-05-29'\n",
    "\n",
    "request=base+datasetid+'&'+locationid+'&'+startdate+'&'+enddate\n",
    "\n",
    "#https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:28801&startdate=2010-05-01&enddate=2010-05-01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[295, 480, 439, 699, 307, 371, 318, 665, 358, 279, 284, 0]\n",
      "338.0\n",
      "[53.96, 53.06]\n",
      "53.510000000000005\n",
      "[71.06, 73.03999999999999]\n",
      "72.05\n"
     ]
    }
   ],
   "source": [
    "#make the api call to NOAA\n",
    "r = requests.get(request, headers={'token':Token})\n",
    "#load the api response as a json\n",
    "d = json.loads(r.text)\n",
    "#get all items in the response which are precip readings\n",
    "prcp = [item['value'] for item in d['results'] if item['datatype']=='PRCP']\n",
    "#get all max/min/average temperature readings\n",
    "#convert from tenths of centigrate to F\n",
    "tavg = [((float(item['value'])/10.0)*1.8)+32 for item in d['results'] if item['datatype']=='TAVG']\n",
    "tmin = [((float(item['value'])/10.0)*1.8)+32 for item in d['results'] if item['datatype']=='TMIN']\n",
    "tmax = [((float(item['value'])/10.0)*1.8)+32 for item in d['results'] if item['datatype']=='TMAX']\n",
    "\n",
    "#print data and median values from list\n",
    "print(prcp)\n",
    "print(median(prcp))\n",
    "\n",
    "print(tmin)\n",
    "print(median(tmin))\n",
    "\n",
    "print(tmax)\n",
    "print(median(tmax))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
